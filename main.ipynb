{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "### Integrantes:\n",
    "\n",
    " - BRANDON JAVIER REYES MORALES\n",
    " - CARLOS ALBERTO VALLADARES GUERRA \n",
    " - JUAN PABLO SOLIS ALBIZUREZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje\n",
    "supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo\n",
    "\n",
    " - El aprendizaje por diferencias dtemporales (TD Learning) es una técnica de parendizaje por refuerzo que mezcla aprendizaje supervizado y no supervizado. \n",
    "\n",
    " - A diferencia del aprendizaje  supervisado tradicional, utiliza un conjunto de datos etiquetados con entrenar modelos, para reforzar la enseñanza, y en particular la capacitación en TD Learning no requiere ejemplos denotados por un comportamiento adecuado o inadecuado. Por otro lado, el agente puede aprender a tomar decisiones óptimas que interactúen con el entorno y recibir comentarios en forma de remuneración o multas.\n",
    "\n",
    " - El 'error de diferencia temporal' Este error es fundamental en los algoritmos de aprendizaje por refuerzo, ya que se utiliza para actualizar las estimaciones de valor de los estados o acciones, permitiendo al agente mejorar su política de decisión con el tiempo.\n",
    "\n",
    "    - ´'El error de diferencia temporal'   = 'la recompensa real obtenida' - 'la recompensa esperada en un estado determinado'\n",
    "\n",
    "| Aspecto                                 | Aprendizaje por Diferencias Temporales (TD Learning) | Aprendizaje Supervisado Tradicional |\n",
    "|-----------------------------------------|------------------------------------------------------|-------------------------------------|\n",
    "| **Tipo de aprendizaje**                 | Aprendizaje por refuerzo                            | Aprendizaje supervisado             |\n",
    "| **Datos de entrenamiento**              | No requiere datos etiquetados; aprende de la interacción con el entorno | Requiere conjuntos de datos etiquetados |\n",
    "| **Retroalimentación**                   | Basada en recompensas o castigos obtenidos de las acciones realizadas | Basada en la comparación con etiquetas conocidas |\n",
    "| **Objetivo principal**                  | Aprender una política óptima para maximizar recompensas acumuladas a largo plazo | Aprender una función que mapea entradas a salidas correctas |\n",
    "| **Actualización de valores**            | Utiliza el error de diferencia temporal para actualizar estimaciones de valor | Minimiza la diferencia entre las predicciones y las etiquetas verdaderas |\n",
    "| **Aplicaciones comunes**                | Juegos, control robótico, sistemas de recomendación  | Clasificación de imágenes, reconocimiento de voz, predicción de valores numéricos |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación\n",
    "\n",
    "\n",
    " - En los juegos simultáneos, he aprendido que los jugadores toman decisiones al mismo tiempo sin conocer las elecciones de sus oponentes. Para manejar esta incertidumbre, suelen utilizar estrategias mixtas, asignando probabilidades a cada posible acción para maximizar su utilidad esperada. Un ejemplo del mundo real es que dos aerolíneas (Aerolínea X y Aerolínea Y) operan en la misma región y deben decidir en qué horario asignarán sus vuelos entre dos ciudades importantes. Ambas aerolíneas deben tomar esta decisión sin conocer la elección de la otra.\n",
    "\n",
    " - Opciones disponibles:Cada aerolínea tiene dos posibles estrategias:\n",
    "\n",
    "    - Programar vuelos en horario pico (mañana y tarde)\n",
    "    - Programar vuelos en horario fuera de pico (noche y madrugada)\n",
    "\n",
    "      | Aerolínea X / Aerolínea Y | Horario Pico | Horario No Pico |\n",
    "      |--------------------------|--------------|----------------|\n",
    "      | **Horario Pico**         | Ambas compiten por pasajeros, reduciendo ganancias | X tiene ventaja, más pasajeros |\n",
    "      | **Horario No Pico**      | Y tiene ventaja, más pasajeros | Ambas tienen menos demanda |\n",
    "\n",
    " - Estrategias posibles:\n",
    "   1. Estrategia conservadora: Elegir el horario no pico para evitar la competencia directa y asegurar una ocupación media constante.\n",
    "   2. Estrategia agresiva: Elegir el horario pico para captar la mayor cantidad de pasajeros, asumiendo que la otra aerolínea evitará la competencia.\n",
    "   3. Estrategia de equilibrio: Distribuir los vuelos en diferentes horarios para minimizar riesgos, esperando que la otra aerolínea haga lo mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Qué distingue los juegos de suma cero de los juegos de suma cero y cómo afecta esta diferencia al\n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren\n",
    "en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas\n",
    "\n",
    " - En la teoría del juego, los juegos de suma cero son aquellos en los que los beneficios del jugador son exactamente iguales a la pérdida de otro, por lo que la cantidad total de excedente y pérdida es cero. Por otro lado, no permita que los juegos cerosos para todos los jugadores al mismo tiempo ganen o pierdan, no exceder y las pérdidas no siempre se agregan a cero. Esto significa que la colaboración entre los jugadores puede conducir a beneficios mutuos, mientras que la competencia puede causar pérdidas por todo.\n",
    "\n",
    " - Un videojuego que entra en la categoría  es 33 Immortals, una cooperativa para múltiples jugadores, acoplando a Roguelike hasta 33 jugadores. En este juego, los participantes deben cooperar para abordar los desafíos y enemigos que enfatizan la importancia de la cooperación para lograr objetivos comunes. La falta de coordinación puede conducir a una falla colectiva, mientras que la cooperación efectiva puede conducir al éxito conjunto.\n",
    "\n",
    " - Consideraciones estratégicas únicas en juegos de suma no cero:\n",
    "\n",
    "    - Cooperación: La posibilidad de que todos los jugadores se beneficien fomenta la colaboración y la formación de alianzas estratégicas.​\n",
    "\n",
    "    - Comunicación: La coordinación efectiva suele requerir una comunicación clara y constante entre los jugadores para alinear estrategias y acciones.\n",
    "    - Confianza: Es fundamental confiar en que los demás jugadores cumplirán con su parte en la estrategia conjunta, lo que puede fortalecer o debilitar las alianzas.​\n",
    "\n",
    "    - Gestión de recursos: La distribución equitativa y eficiente de recursos entre los jugadores es crucial para maximizar los beneficios colectivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
    "\n",
    " - El equilibrio de Nash es el concepto de la teoría del juego, que se relaciona con situaciones en las que varios jugadores toman decisiones al mismo tiempo sin conocer la elección de los demás. Este equilibrio se logra si cada jugador elige la mejor estrategia posible, teniendo en cuenta las decisiones de los demás, por lo que nadie tiene un incentivo para cambiar su elección, ya que dichos cambios no mejorará sus resultados. Por ejemplo, el dilema del prisionero reconoce a ambos jugadores porque nadie sabe que no puede mejorarse cambiando la decisión sobre uno. Este equilibrio es una solución estable, ya que cualquier desviación individual crearía una herramienta más baja para un jugador que decida cambiar su estrategia y alentarlo a mantener su elección original.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
    "\n",
    "\n",
    " - El aprendizaje por diferencias temporales (Temporal Difference Learning) es una técnica del aprendizaje por refuerzo que permite a un agente aprender y optimizar su comportamiento mediante la interacción continua con un entorno dinámico, sin requerir un modelo previo de este. Esta metodología es especialmente útil en procesos de toma de decisiones donde las condiciones cambian con el tiempo, ya que el agente ajusta sus estrategias basándose en la retroalimentación recibida de sus acciones anteriores. ​\n",
    "\n",
    " - Uno de los desafíos clave en la implementación del aprendizaje por diferencias temporales es gestionar el equilibrio entre exploración y explotación. La exploración implica que el agente pruebe nuevas acciones para descubrir potenciales estrategias más efectivas, mientras que la explotación se refiere a la utilización de acciones ya conocidas que proporcionan recompensas satisfactorias. Lograr este equilibrio es crucial para evitar que el agente se estanque en soluciones subóptimas o, por el contrario, desperdicie recursos explorando excesivamente sin aprovechar las estrategias ya efectivas. ​\n",
    "\n",
    " - Entre los desafíos asociados con la implementación práctica del aprendizaje por diferencias temporales se encuentran la necesidad de seleccionar adecuadamente parámetros como la tasa de aprendizaje y el factor de descuento, que influyen en la rapidez y estabilidad del aprendizaje. Además, en entornos complejos, la dimensionalidad del espacio de estados puede ser elevada, lo que complica la representación y actualización de las estimaciones de valor. Para abordar este problema, se suelen emplear técnicas de aproximación de funciones, como las redes neuronales, aunque su integración añade complejidad al proceso de aprendizaje. \n",
    "\n",
    "| Aspecto                          | Exploración                                                                                      | Explotación                                                                                       |\n",
    "|----------------------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **Definición**                   | Probar nuevas acciones para descubrir estrategias potencialmente más efectivas.                  | Utilizar acciones ya conocidas que han proporcionado recompensas satisfactorias en el pasado.     |\n",
    "| **Objetivo**                     | Descubrir nuevas oportunidades que puedan mejorar el rendimiento futuro del agente.               | Maximizar las recompensas inmediatas basándose en el conocimiento actual del agente.               |\n",
    "| **Ventajas**                     | Posibilidad de encontrar soluciones más óptimas y evitar estancarse en estrategias subóptimas.   | Aprovechamiento eficiente de estrategias ya comprobadas como efectivas.                            |\n",
    "| **Desventajas**                  | Riesgo de gastar recursos en acciones que no aporten mejoras significativas.                     | Posibilidad de perder oportunidades de mejora al no considerar nuevas estrategias.                 |\n",
    "| **Equilibrio entre ambas**       | Es esencial balancear exploración y explotación para evitar tanto la falta de mejora como el estancamiento en estrategias subóptimas. |\n",
    "| **Desafíos en la implementación**| Determinar la proporción adecuada entre exploración y explotación; seleccionar parámetros como la tasa de aprendizaje y el factor de descuento; gestionar la alta dimensionalidad en entornos complejos. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del torneo:\n",
      "{'TD': 0, 'Minimax': 0, 'MinimaxAB': 0, 'Empate': 150}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "FILAS = 6\n",
    "COLUMNAS = 7\n",
    "\n",
    "def crear_tablero():\n",
    "    return np.zeros((FILAS, COLUMNAS), dtype=int)\n",
    "\n",
    "def columna_valida(tablero, col):\n",
    "    return tablero[FILAS-1, col] == 0\n",
    "\n",
    "def obtener_fila_libre(tablero, col):\n",
    "    for f in range(FILAS):\n",
    "        if tablero[f, col] == 0:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "def colocar_ficha(tablero, fila, col, ficha):\n",
    "    tablero[fila, col] = ficha\n",
    "\n",
    "def hay_ganador(tablero, ficha):\n",
    "    # Horizontal\n",
    "    for r in range(FILAS):\n",
    "        for c in range(COLUMNAS - 3):\n",
    "            if all(tablero[r, c+i] == ficha for i in range(4)):\n",
    "                return True\n",
    "    # Vertical\n",
    "    for c in range(COLUMNAS):\n",
    "        for r in range(FILAS - 3):\n",
    "            if all(tablero[r+i, c] == ficha for i in range(4)):\n",
    "                return True\n",
    "    # Diagonal positiva\n",
    "    for r in range(FILAS - 3):\n",
    "        for c in range(COLUMNAS - 3):\n",
    "            if all(tablero[r+i, c+i] == ficha for i in range(4)):\n",
    "                return True\n",
    "    # Diagonal negativa\n",
    "    for r in range(3, FILAS):\n",
    "        for c in range(COLUMNAS - 3):\n",
    "            if all(tablero[r-i, c+i] == ficha for i in range(4)):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def tablero_lleno(tablero):\n",
    "    return all(tablero[FILAS-1, c] != 0 for c in range(COLUMNAS))\n",
    "\n",
    "def columnas_disponibles(tablero):\n",
    "    return [c for c in range(COLUMNAS) if columna_valida(tablero, c)]\n",
    "\n",
    "def puntuar_tablero(tablero, ficha):\n",
    "    # Para la simulación (minimax), se usa la evaluación simple del tablero\n",
    "    # Esta función proviene de tu Lab6 (adaptada)\n",
    "    def evaluar_ventana(ventana, ficha):\n",
    "        puntuacion = 0\n",
    "        ficha_oponente = 1 if ficha == 2 else 2\n",
    "        if ventana.count(ficha) == 4:\n",
    "            puntuacion += 100\n",
    "        elif ventana.count(ficha) == 3 and ventana.count(0) == 1:\n",
    "            puntuacion += 10\n",
    "        elif ventana.count(ficha) == 2 and ventana.count(0) == 2:\n",
    "            puntuacion += 2\n",
    "        if ventana.count(ficha_oponente) == 3 and ventana.count(0) == 1:\n",
    "            puntuacion -= 4\n",
    "        return puntuacion\n",
    "\n",
    "    score = 0\n",
    "    # Extra por controlar la columna central\n",
    "    columna_central = [int(i) for i in list(tablero[:, COLUMNAS//2])]\n",
    "    score += columna_central.count(ficha) * 3\n",
    "\n",
    "    # Revisar filas\n",
    "    for r in range(FILAS):\n",
    "        fila = [int(x) for x in tablero[r, :]]\n",
    "        for c in range(COLUMNAS - 3):\n",
    "            ventana = fila[c:c+4]\n",
    "            score += evaluar_ventana(ventana, ficha)\n",
    "    # Revisar columnas\n",
    "    for c in range(COLUMNAS):\n",
    "        col = [int(x) for x in tablero[:, c]]\n",
    "        for r in range(FILAS - 3):\n",
    "            ventana = col[r:r+4]\n",
    "            score += evaluar_ventana(ventana, ficha)\n",
    "    # Diagonales positivas\n",
    "    for r in range(FILAS - 3):\n",
    "        for c in range(COLUMNAS - 3):\n",
    "            ventana = [tablero[r+i, c+i] for i in range(4)]\n",
    "            score += evaluar_ventana(ventana, ficha)\n",
    "    # Diagonales negativas\n",
    "    for r in range(3, FILAS):\n",
    "        for c in range(COLUMNAS - 3):\n",
    "            ventana = [tablero[r-i, c+i] for i in range(4)]\n",
    "            score += evaluar_ventana(ventana, ficha)\n",
    "    return score\n",
    "\n",
    "\n",
    "def minimax(tablero, profundidad, maximizando, ficha, alpha=-math.inf, beta=math.inf, usar_poda=True):\n",
    "    if profundidad == 0 or hay_ganador(tablero, 1) or hay_ganador(tablero, 2) or tablero_lleno(tablero):\n",
    "        if hay_ganador(tablero, ficha):\n",
    "            return (None, math.inf)\n",
    "        elif hay_ganador(tablero, 1 if ficha == 2 else 2):\n",
    "            return (None, -math.inf)\n",
    "        else:\n",
    "            return (None, puntuar_tablero(tablero, ficha))\n",
    "    validas = columnas_disponibles(tablero)\n",
    "    if maximizando:\n",
    "        valor = -math.inf\n",
    "        mejor_col = random.choice(validas)\n",
    "        for col in validas:\n",
    "            fila_libre = obtener_fila_libre(tablero, col)\n",
    "            copia = tablero.copy()\n",
    "            colocar_ficha(copia, fila_libre, col, ficha)\n",
    "            _, nuevo_valor = minimax(copia, profundidad-1, False, ficha, alpha, beta, usar_poda)\n",
    "            if nuevo_valor > valor:\n",
    "                valor = nuevo_valor\n",
    "                mejor_col = col\n",
    "            if usar_poda:\n",
    "                alpha = max(alpha, valor)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "        return (mejor_col, valor)\n",
    "    else:\n",
    "        valor = math.inf\n",
    "        mejor_col = random.choice(validas)\n",
    "        ficha_oponente = 1 if ficha == 2 else 2\n",
    "        for col in validas:\n",
    "            fila_libre = obtener_fila_libre(tablero, col)\n",
    "            copia = tablero.copy()\n",
    "            colocar_ficha(copia, fila_libre, col, ficha_oponente)\n",
    "            _, nuevo_valor = minimax(copia, profundidad-1, True, ficha, alpha, beta, usar_poda)\n",
    "            if nuevo_valor < valor:\n",
    "                valor = nuevo_valor\n",
    "                mejor_col = col\n",
    "            if usar_poda:\n",
    "                beta = min(beta, valor)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "        return (mejor_col, valor)\n",
    "\n",
    "def movimiento_ia(tablero, profundidad, ficha, usar_poda=True):\n",
    "    col, _ = minimax(tablero, profundidad, True, ficha, usar_poda=usar_poda)\n",
    "    if col is None or not columna_valida(tablero, col):\n",
    "        col = random.choice(columnas_disponibles(tablero))\n",
    "    return col\n",
    "\n",
    "\n",
    "\n",
    "# Agentes para el torneo\n",
    "\n",
    "\n",
    "# --- Agente TD Learning (Q-learning) ---\n",
    "class TDAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.alpha = alpha      # tasa de aprendizaje\n",
    "        self.gamma = gamma      # factor de descuento\n",
    "        self.epsilon = epsilon  # tasa de exploración\n",
    "        self.q_table = {}       # diccionario para almacenar Q(s,a)\n",
    "\n",
    "    def get_state_key(self, tablero):\n",
    "        # Convertir el estado (array NumPy) a una tupla de tuplas\n",
    "        return tuple(tuple(fila) for fila in tablero)\n",
    "\n",
    "    def get_q(self, state_key, accion):\n",
    "        return self.q_table.get((state_key, accion), 0.0)\n",
    "\n",
    "    def set_q(self, state_key, accion, valor):\n",
    "        self.q_table[(state_key, accion)] = valor\n",
    "\n",
    "    def choose_action(self, tablero, acciones_validas):\n",
    "        state_key = self.get_state_key(tablero)\n",
    "        # Estrategia epsilon-greedy: con probabilidad epsilon se explora\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(acciones_validas)\n",
    "        # Sino, se selecciona la acción con mayor Q\n",
    "        q_values = {accion: self.get_q(state_key, accion) for accion in acciones_validas}\n",
    "        max_q = max(q_values.values())\n",
    "        # En caso de empate, se elige al azar entre las de máximo Q\n",
    "        mejores = [accion for accion, q in q_values.items() if q == max_q]\n",
    "        return random.choice(mejores)\n",
    "\n",
    "    def update(self, tablero, accion, recompensa, next_tablero, next_acciones_validas, done):\n",
    "        state_key = self.get_state_key(tablero)\n",
    "        current_q = self.get_q(state_key, accion)\n",
    "        if done:\n",
    "            objetivo = recompensa\n",
    "        else:\n",
    "            next_state_key = self.get_state_key(next_tablero)\n",
    "            next_qs = [self.get_q(next_state_key, a) for a in next_acciones_validas]\n",
    "            objetivo = recompensa + self.gamma * max(next_qs, default=0)\n",
    "        nuevo_q = current_q + self.alpha * (objetivo - current_q)\n",
    "        self.set_q(state_key, accion, nuevo_q)\n",
    "\n",
    "    # Método para que el agente decida movimiento (para uniformidad en el torneo)\n",
    "    def get_move(self, tablero):\n",
    "        acciones = columnas_disponibles(tablero)\n",
    "        return self.choose_action(tablero, acciones)\n",
    "\n",
    "# --- Agente Minimax ---\n",
    "class MinimaxAgent:\n",
    "    def __init__(self, profundidad=4, usar_poda=False):\n",
    "        self.profundidad = profundidad\n",
    "        self.usar_poda = usar_poda\n",
    "\n",
    "    def get_move(self, tablero, ficha):\n",
    "        return movimiento_ia(tablero, self.profundidad, ficha, self.usar_poda)\n",
    "\n",
    "\n",
    "# Función para simular una partida entre dos agentes\n",
    "def simulate_game(agente1, agente2, ficha1, ficha2, td_agent=None, entrenamiento_td=False):\n",
    "    \"\"\"\n",
    "    Simula una partida entre dos agentes en la que:\n",
    "      - agente1 juega con ficha1\n",
    "      - agente2 juega con ficha2\n",
    "    Si 'td_agent' no es None, se espera que alguno de los jugadores sea el agente TD (para actualizar Q).\n",
    "    La variable 'entrenamiento_td' indica si se realizan actualizaciones TD tras cada movimiento.\n",
    "    Retorna: 1 si gana agente1, 2 si gana agente2, 0 si empate.\n",
    "    \"\"\"\n",
    "    tablero = crear_tablero()\n",
    "    turno = 0  # 0: turno del jugador 1, 1: turno del jugador 2\n",
    "\n",
    "    # Para almacenar el estado anterior y acción (sólo para TD)\n",
    "    estado_anterior = None\n",
    "    accion_anterior = None\n",
    "    jugador_td = None\n",
    "    if td_agent is not None:\n",
    "        # Determinamos si agente1 o agente2 es el TD\n",
    "        if isinstance(agente1, TDAgent):\n",
    "            jugador_td = 1\n",
    "        elif isinstance(agente2, TDAgent):\n",
    "            jugador_td = 2\n",
    "\n",
    "    while True:\n",
    "        acciones_validas = columnas_disponibles(tablero)\n",
    "        if turno == 0:\n",
    "            # Turno del jugador 1\n",
    "            if isinstance(agente1, TDAgent):\n",
    "                accion = agente1.get_move(tablero)\n",
    "            else:\n",
    "                # Para Minimax se requiere pasar la ficha que utiliza\n",
    "                accion = agente1.get_move(tablero, ficha1)\n",
    "        else:\n",
    "            if isinstance(agente2, TDAgent):\n",
    "                accion = agente2.get_move(tablero)\n",
    "            else:\n",
    "                accion = agente2.get_move(tablero, ficha2)\n",
    "\n",
    "        # Guardamos estado y acción si este jugador es el TD y estamos en entrenamiento\n",
    "        if entrenamiento_td and td_agent is not None:\n",
    "            if (turno == 0 and jugador_td == 1) or (turno == 1 and jugador_td == 2):\n",
    "                estado_anterior = tablero.copy()\n",
    "                accion_anterior = accion\n",
    "\n",
    "        if accion not in acciones_validas:\n",
    "            # Movimiento inválido; se asigna aleatorio\n",
    "            accion = random.choice(acciones_validas)\n",
    "\n",
    "        fila = obtener_fila_libre(tablero, accion)\n",
    "        # Asigna ficha según turno\n",
    "        ficha_actual = ficha1 if turno == 0 else ficha2\n",
    "        colocar_ficha(tablero, fila, accion, ficha_actual)\n",
    "\n",
    "        # Verificar si hay ganador\n",
    "        if hay_ganador(tablero, ficha_actual):\n",
    "            # Asignar recompensas: +1 al ganador, -1 al perdedor\n",
    "            if entrenamiento_td and td_agent is not None:\n",
    "                recompensa = 1 if ((turno == 0 and jugador_td == 1) or (turno == 1 and jugador_td == 2)) else -1\n",
    "                # Actualizar el agente TD con estado terminal (no hay siguiente estado)\n",
    "                td_agent.update(estado_anterior, accion_anterior, recompensa, tablero, columnas_disponibles(tablero), done=True)\n",
    "            return (1 if turno == 0 else 2), tablero\n",
    "\n",
    "\n",
    "        if tablero_lleno(tablero):\n",
    "            # Empate: recompensa 0 para ambos\n",
    "            if entrenamiento_td and td_agent is not None:\n",
    "                td_agent.update(estado_anterior, accion_anterior, 0, tablero, columnas_disponibles(tablero), done=True)\n",
    "            return 0, tablero\n",
    "\n",
    "\n",
    "        # Si es TD y es su turno, actualizar con recompensa intermedia = 0 (movimiento normal)\n",
    "        if entrenamiento_td and td_agent is not None:\n",
    "            if (turno == 0 and jugador_td == 1) or (turno == 1 and jugador_td == 2):\n",
    "                # Actualización con recompensa 0 y estado siguiente\n",
    "                td_agent.update(estado_anterior, accion_anterior, 0, tablero, columnas_disponibles(tablero), done=False)\n",
    "\n",
    "        turno = 1 - turno  # alterna turno\n",
    "\n",
    "\n",
    "# Simulación del torneo\n",
    "def torneo():\n",
    "    num_partidas = 50\n",
    "\n",
    "    # Crear instancias de los agentes\n",
    "    # Agente TD: con parámetros que se pueden ajustar (alpha, gamma, epsilon)\n",
    "    td_agent = TDAgent(alpha=0.1, gamma=0.9, epsilon=0.05)\n",
    "    # Agente Minimax sin poda\n",
    "    minimax_agent = MinimaxAgent(profundidad=4, usar_poda=False)\n",
    "    # Agente Minimax con poda alfa-beta\n",
    "    minimax_ab_agent = MinimaxAgent(profundidad=5, usar_poda=True)\n",
    "\n",
    "    # Contadores de victorias\n",
    "    victorias = {\"TD\": 0, \"Minimax\": 0, \"MinimaxAB\": 0, \"Empate\": 0}\n",
    "\n",
    "    # --- Partido 1: TD vs Minimax ---\n",
    "    for i in range(num_partidas):\n",
    "        # Asumimos que TD juega con ficha 1 y Minimax con ficha 2 (se puede alternar si se desea)\n",
    "        ganador = simulate_game(td_agent, minimax_agent, ficha1=1, ficha2=2, td_agent=td_agent, entrenamiento_td=True)\n",
    "        if ganador == 1:\n",
    "            victorias[\"TD\"] += 1\n",
    "        elif ganador == 2:\n",
    "            victorias[\"Minimax\"] += 1\n",
    "        else:\n",
    "            victorias[\"Empate\"] += 1\n",
    "\n",
    "    # --- Partido 2: TD vs MinimaxAB ---\n",
    "    for i in range(num_partidas):\n",
    "        ganador = simulate_game(td_agent, minimax_ab_agent, ficha1=1, ficha2=2, td_agent=td_agent, entrenamiento_td=True)\n",
    "        if ganador == 1:\n",
    "            victorias[\"TD\"] += 1\n",
    "        elif ganador == 2:\n",
    "            victorias[\"MinimaxAB\"] += 1\n",
    "        else:\n",
    "            victorias[\"Empate\"] += 1\n",
    "\n",
    "    # --- Partido 3: Minimax vs MinimaxAB ---\n",
    "    for i in range(num_partidas):\n",
    "        ganador = simulate_game(minimax_agent, minimax_ab_agent, ficha1=1, ficha2=2, td_agent=None, entrenamiento_td=False)\n",
    "        if ganador == 1:\n",
    "            victorias[\"Minimax\"] += 1\n",
    "        elif ganador == 2:\n",
    "            victorias[\"MinimaxAB\"] += 1\n",
    "        else:\n",
    "            victorias[\"Empate\"] += 1\n",
    "\n",
    "    return victorias\n",
    "\n",
    "\n",
    "# Graficar resultados del torneo\n",
    "def graficar_resultados(victorias):\n",
    "    agentes = [ \"TD\", \"Minimax\", \"MinimaxAB\"]\n",
    "    # Se excluyen los empates de la gráfica\n",
    "    resultados = [victorias[agente] for agente in agentes]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(agentes, resultados, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n",
    "    plt.xlabel(\"Agente\")\n",
    "    plt.ylabel(\"Número de victorias\")\n",
    "    plt.title(\"Resultados del torneo (50 partidas por enfrentamiento)\")\n",
    "    plt.ylim(0, max(resultados) + 10)\n",
    "    for i, v in enumerate(resultados):\n",
    "        plt.text(i, v + 1, str(v), ha='center', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Función ejecutar la simulación\n",
    "def main():\n",
    "    resultados = torneo()\n",
    "    print(\"Resultados del torneo:\")\n",
    "    print(resultados)\n",
    "    graficar_resultados(resultados)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Configuración visual\n",
    "TAM_CELDA = 100\n",
    "MARGEN_CIRCULO = 5\n",
    "COLOR_FONDO = \"blue\"\n",
    "COLOR_VACIO = \"white\"\n",
    "COLOR_JUGADOR = \"red\"     # Ficha 1\n",
    "COLOR_IA = \"yellow\"        # Ficha 2\n",
    "\n",
    "# Clase para mostrar el tablero final con leyenda y ganador\n",
    "class VistaTableroFinal:\n",
    "    def __init__(self, root, tablero, titulo, agente1_nombre, agente2_nombre, ganador):\n",
    "        self.root = root\n",
    "        self.tablero = tablero\n",
    "        self.root.title(titulo)\n",
    "\n",
    "        # Leyenda de colores\n",
    "        leyenda = f\"{agente1_nombre} = ROJO     |     {agente2_nombre} = AMARILLO\"\n",
    "        self.lbl_info = tk.Label(self.root, text=leyenda, font=(\"Arial\", 14), pady=5)\n",
    "        self.lbl_info.pack()\n",
    "\n",
    "        # Resultado de la partida\n",
    "        if ganador == 1:\n",
    "            resultado = f\"Ganador: {agente1_nombre} 🎉\"\n",
    "        elif ganador == 2:\n",
    "            resultado = f\"Ganador: {agente2_nombre} 🎉\"\n",
    "        else:\n",
    "            resultado = \"¡Empate!\"\n",
    "\n",
    "        self.lbl_resultado = tk.Label(self.root, text=resultado, font=(\"Arial\", 16, \"bold\"), fg=\"green\", pady=5)\n",
    "        self.lbl_resultado.pack()\n",
    "\n",
    "        # Canvas del tablero\n",
    "        ancho = COLUMNAS * TAM_CELDA\n",
    "        alto = FILAS * TAM_CELDA\n",
    "        self.canvas = tk.Canvas(self.root, width=ancho, height=alto, bg=COLOR_FONDO)\n",
    "        self.canvas.pack()\n",
    "        self.dibujar_tablero()\n",
    "\n",
    "    def dibujar_tablero(self):\n",
    "        for f in range(FILAS):\n",
    "            for c in range(COLUMNAS):\n",
    "                x1 = c * TAM_CELDA\n",
    "                y1 = (FILAS - 1 - f) * TAM_CELDA\n",
    "                x2 = x1 + TAM_CELDA\n",
    "                y2 = y1 + TAM_CELDA\n",
    "\n",
    "                valor = self.tablero[f, c]\n",
    "                if valor == 0:\n",
    "                    color = COLOR_VACIO\n",
    "                elif valor == 1:\n",
    "                    color = COLOR_JUGADOR\n",
    "                else:\n",
    "                    color = COLOR_IA\n",
    "\n",
    "                self.canvas.create_rectangle(x1, y1, x2, y2, fill=COLOR_FONDO, outline=\"black\")\n",
    "                self.canvas.create_oval(\n",
    "                    x1 + MARGEN_CIRCULO, y1 + MARGEN_CIRCULO,\n",
    "                    x2 - MARGEN_CIRCULO, y2 - MARGEN_CIRCULO,\n",
    "                    fill=color\n",
    "                )\n",
    "\n",
    "# Mostrar tablero final con leyenda y ganador\n",
    "def mostrar_tablero_final(tablero, titulo, agente1, agente2, ganador):\n",
    "    root = tk.Tk()\n",
    "    VistaTableroFinal(root, tablero, titulo, agente1, agente2, ganador)\n",
    "    root.mainloop()\n",
    "\n",
    "# Crear agentes\n",
    "td_agent = TDAgent(alpha=0.1, gamma=0.9, epsilon=0.05)\n",
    "minimax_agent = MinimaxAgent(profundidad=4, usar_poda=False)\n",
    "minimax_ab_agent = MinimaxAgent(profundidad=6, usar_poda=True)\n",
    "\n",
    "# Partida 1: TD vs Minimax\n",
    "ganador1, tablero1 = simulate_game(td_agent, minimax_agent, ficha1=1, ficha2=2, td_agent=td_agent, entrenamiento_td=True)\n",
    "mostrar_tablero_final(tablero1, \"Partida 1: TD vs Minimax\", \"TD Agent\", \"Minimax Agent\", ganador1)\n",
    "\n",
    "# Partida 2: TD vs Minimax AB\n",
    "ganador2, tablero2 = simulate_game(td_agent, minimax_ab_agent, ficha1=1, ficha2=2, td_agent=td_agent, entrenamiento_td=True)\n",
    "mostrar_tablero_final(tablero2, \"Partida 2: TD vs Minimax AB\", \"TD Agent\", \"Minimax AB Agent\", ganador2)\n",
    "\n",
    "# Partida 3: Minimax vs Minimax AB\n",
    "ganador3, tablero3 = simulate_game(minimax_ab_agent, minimax_agent, ficha1=1, ficha2=2)\n",
    "mostrar_tablero_final(tablero3, \"Partida 3: Minimax AB vs Minimax\", \"Minimax AB Agent\", \"Minimax Agent\", ganador3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias:\n",
    " - Nash, J. (1950). Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36(1), 48-49. Recuperado de https://www.pnas.org/doi/10.1073/pnas.36.1.48\n",
    "\n",
    " - Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press. Recuperado de https://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    " - Wikipedia contributors. (2023, January 15). Dilema del prisionero. En Wikipedia, La enciclopedia libre. Recuperado de https://es.wikipedia.org/wiki/Dilema_del_prisionero\n",
    "\n",
    " - Wikipedia contributors. (2023, January 15). Teoría de juegos. En Wikipedia, La enciclopedia libre. Recuperado de https://es.wikipedia.org/wiki/Teor%C3%ADa_de_juegos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
