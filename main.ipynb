{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "### Integrantes:\n",
    "\n",
    " - BRANDON JAVIER REYES MORALES\n",
    " - CARLOS ALBERTO VALLADARES GUERRA \n",
    " - JUAN PABLO SOLIS ALBIZUREZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje\n",
    "supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo\n",
    "\n",
    " - El aprendizaje por diferencias dtemporales (TD Learning) es una técnica de parendizaje por refuerzo que mezcla aprendizaje supervizado y no supervizado. \n",
    "\n",
    " - A diferencia del aprendizaje  supervisado tradicional, utiliza un conjunto de datos etiquetados con entrenar modelos, para reforzar la enseñanza, y en particular la capacitación en TD Learning no requiere ejemplos denotados por un comportamiento adecuado o inadecuado. Por otro lado, el agente puede aprender a tomar decisiones óptimas que interactúen con el entorno y recibir comentarios en forma de remuneración o multas.\n",
    "\n",
    " - El 'error de diferencia temporal' Este error es fundamental en los algoritmos de aprendizaje por refuerzo, ya que se utiliza para actualizar las estimaciones de valor de los estados o acciones, permitiendo al agente mejorar su política de decisión con el tiempo.\n",
    "\n",
    "    - ´'El error de diferencia temporal'   = 'la recompensa real obtenida' - 'la recompensa esperada en un estado determinado'\n",
    "\n",
    "| Aspecto                                 | Aprendizaje por Diferencias Temporales (TD Learning) | Aprendizaje Supervisado Tradicional |\n",
    "|-----------------------------------------|------------------------------------------------------|-------------------------------------|\n",
    "| **Tipo de aprendizaje**                 | Aprendizaje por refuerzo                            | Aprendizaje supervisado             |\n",
    "| **Datos de entrenamiento**              | No requiere datos etiquetados; aprende de la interacción con el entorno | Requiere conjuntos de datos etiquetados |\n",
    "| **Retroalimentación**                   | Basada en recompensas o castigos obtenidos de las acciones realizadas | Basada en la comparación con etiquetas conocidas |\n",
    "| **Objetivo principal**                  | Aprender una política óptima para maximizar recompensas acumuladas a largo plazo | Aprender una función que mapea entradas a salidas correctas |\n",
    "| **Actualización de valores**            | Utiliza el error de diferencia temporal para actualizar estimaciones de valor | Minimiza la diferencia entre las predicciones y las etiquetas verdaderas |\n",
    "| **Aplicaciones comunes**                | Juegos, control robótico, sistemas de recomendación  | Clasificación de imágenes, reconocimiento de voz, predicción de valores numéricos |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación\n",
    "\n",
    "\n",
    " - En los juegos simultáneos, he aprendido que los jugadores toman decisiones al mismo tiempo sin conocer las elecciones de sus oponentes. Para manejar esta incertidumbre, suelen utilizar estrategias mixtas, asignando probabilidades a cada posible acción para maximizar su utilidad esperada. Un ejemplo del mundo real es que dos aerolíneas (Aerolínea X y Aerolínea Y) operan en la misma región y deben decidir en qué horario asignarán sus vuelos entre dos ciudades importantes. Ambas aerolíneas deben tomar esta decisión sin conocer la elección de la otra.\n",
    "\n",
    " - Opciones disponibles:Cada aerolínea tiene dos posibles estrategias:\n",
    "\n",
    "    - Programar vuelos en horario pico (mañana y tarde)\n",
    "    - Programar vuelos en horario fuera de pico (noche y madrugada)\n",
    "\n",
    "      | Aerolínea X / Aerolínea Y | Horario Pico | Horario No Pico |\n",
    "      |--------------------------|--------------|----------------|\n",
    "      | **Horario Pico**         | Ambas compiten por pasajeros, reduciendo ganancias | X tiene ventaja, más pasajeros |\n",
    "      | **Horario No Pico**      | Y tiene ventaja, más pasajeros | Ambas tienen menos demanda |\n",
    "\n",
    " - Estrategias posibles:\n",
    "   1. Estrategia conservadora: Elegir el horario no pico para evitar la competencia directa y asegurar una ocupación media constante.\n",
    "   2. Estrategia agresiva: Elegir el horario pico para captar la mayor cantidad de pasajeros, asumiendo que la otra aerolínea evitará la competencia.\n",
    "   3. Estrategia de equilibrio: Distribuir los vuelos en diferentes horarios para minimizar riesgos, esperando que la otra aerolínea haga lo mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Qué distingue los juegos de suma cero de los juegos de suma cero y cómo afecta esta diferencia al\n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren\n",
    "en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas\n",
    "\n",
    " - En la teoría del juego, los juegos de suma cero son aquellos en los que los beneficios del jugador son exactamente iguales a la pérdida de otro, por lo que la cantidad total de excedente y pérdida es cero. Por otro lado, no permita que los juegos cerosos para todos los jugadores al mismo tiempo ganen o pierdan, no exceder y las pérdidas no siempre se agregan a cero. Esto significa que la colaboración entre los jugadores puede conducir a beneficios mutuos, mientras que la competencia puede causar pérdidas por todo.\n",
    "\n",
    " - Un videojuego que entra en la categoría  es 33 Immortals, una cooperativa para múltiples jugadores, acoplando a Roguelike hasta 33 jugadores. En este juego, los participantes deben cooperar para abordar los desafíos y enemigos que enfatizan la importancia de la cooperación para lograr objetivos comunes. La falta de coordinación puede conducir a una falla colectiva, mientras que la cooperación efectiva puede conducir al éxito conjunto.\n",
    "\n",
    " - Consideraciones estratégicas únicas en juegos de suma no cero:\n",
    "\n",
    "    - Cooperación: La posibilidad de que todos los jugadores se beneficien fomenta la colaboración y la formación de alianzas estratégicas.​\n",
    "\n",
    "    - Comunicación: La coordinación efectiva suele requerir una comunicación clara y constante entre los jugadores para alinear estrategias y acciones.\n",
    "    - Confianza: Es fundamental confiar en que los demás jugadores cumplirán con su parte en la estrategia conjunta, lo que puede fortalecer o debilitar las alianzas.​\n",
    "\n",
    "    - Gestión de recursos: La distribución equitativa y eficiente de recursos entre los jugadores es crucial para maximizar los beneficios colectivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
    "\n",
    " - El equilibrio de Nash es el concepto de la teoría del juego, que se relaciona con situaciones en las que varios jugadores toman decisiones al mismo tiempo sin conocer la elección de los demás. Este equilibrio se logra si cada jugador elige la mejor estrategia posible, teniendo en cuenta las decisiones de los demás, por lo que nadie tiene un incentivo para cambiar su elección, ya que dichos cambios no mejorará sus resultados. Por ejemplo, el dilema del prisionero reconoce a ambos jugadores porque nadie sabe que no puede mejorarse cambiando la decisión sobre uno. Este equilibrio es una solución estable, ya que cualquier desviación individual crearía una herramienta más baja para un jugador que decida cambiar su estrategia y alentarlo a mantener su elección original.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
    "\n",
    "\n",
    " - El aprendizaje por diferencias temporales (Temporal Difference Learning) es una técnica del aprendizaje por refuerzo que permite a un agente aprender y optimizar su comportamiento mediante la interacción continua con un entorno dinámico, sin requerir un modelo previo de este. Esta metodología es especialmente útil en procesos de toma de decisiones donde las condiciones cambian con el tiempo, ya que el agente ajusta sus estrategias basándose en la retroalimentación recibida de sus acciones anteriores. ​\n",
    "\n",
    " - Uno de los desafíos clave en la implementación del aprendizaje por diferencias temporales es gestionar el equilibrio entre exploración y explotación. La exploración implica que el agente pruebe nuevas acciones para descubrir potenciales estrategias más efectivas, mientras que la explotación se refiere a la utilización de acciones ya conocidas que proporcionan recompensas satisfactorias. Lograr este equilibrio es crucial para evitar que el agente se estanque en soluciones subóptimas o, por el contrario, desperdicie recursos explorando excesivamente sin aprovechar las estrategias ya efectivas. ​\n",
    "\n",
    " - Entre los desafíos asociados con la implementación práctica del aprendizaje por diferencias temporales se encuentran la necesidad de seleccionar adecuadamente parámetros como la tasa de aprendizaje y el factor de descuento, que influyen en la rapidez y estabilidad del aprendizaje. Además, en entornos complejos, la dimensionalidad del espacio de estados puede ser elevada, lo que complica la representación y actualización de las estimaciones de valor. Para abordar este problema, se suelen emplear técnicas de aproximación de funciones, como las redes neuronales, aunque su integración añade complejidad al proceso de aprendizaje. \n",
    "\n",
    "| Aspecto                          | Exploración                                                                                      | Explotación                                                                                       |\n",
    "|----------------------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **Definición**                   | Probar nuevas acciones para descubrir estrategias potencialmente más efectivas.                  | Utilizar acciones ya conocidas que han proporcionado recompensas satisfactorias en el pasado.     |\n",
    "| **Objetivo**                     | Descubrir nuevas oportunidades que puedan mejorar el rendimiento futuro del agente.               | Maximizar las recompensas inmediatas basándose en el conocimiento actual del agente.               |\n",
    "| **Ventajas**                     | Posibilidad de encontrar soluciones más óptimas y evitar estancarse en estrategias subóptimas.   | Aprovechamiento eficiente de estrategias ya comprobadas como efectivas.                            |\n",
    "| **Desventajas**                  | Riesgo de gastar recursos en acciones que no aporten mejoras significativas.                     | Posibilidad de perder oportunidades de mejora al no considerar nuevas estrategias.                 |\n",
    "| **Equilibrio entre ambas**       | Es esencial balancear exploración y explotación para evitar tanto la falta de mejora como el estancamiento en estrategias subóptimas. |\n",
    "| **Desafíos en la implementación**| Determinar la proporción adecuada entre exploración y explotación; seleccionar parámetros como la tasa de aprendizaje y el factor de descuento; gestionar la alta dimensionalidad en entornos complejos. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias:\n",
    " - Nash, J. (1950). Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36(1), 48-49. Recuperado de https://www.pnas.org/doi/10.1073/pnas.36.1.48\n",
    "\n",
    " - Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press. Recuperado de https://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    " - Wikipedia contributors. (2023, January 15). Dilema del prisionero. En Wikipedia, La enciclopedia libre. Recuperado de https://es.wikipedia.org/wiki/Dilema_del_prisionero\n",
    "\n",
    " - Wikipedia contributors. (2023, January 15). Teoría de juegos. En Wikipedia, La enciclopedia libre. Recuperado de https://es.wikipedia.org/wiki/Teor%C3%ADa_de_juegos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
